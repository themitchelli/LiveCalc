{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiveCalc GPU API Server - Google Colab\n",
    "\n",
    "This notebook runs a FastAPI server on Google Colab with GPU acceleration.\n",
    "It exposes REST endpoints for submitting projection jobs remotely from VS Code.\n",
    "\n",
    "**Features:**\n",
    "- FastAPI REST endpoints: /submit, /status, /results, /health\n",
    "- ngrok tunnel for public HTTPS access\n",
    "- Asynchronous job processing in background\n",
    "- GPU acceleration via Numba CUDA\n",
    "- Automatic cleanup after 1 hour\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells in order\n",
    "2. Copy the ngrok URL from output\n",
    "3. Configure VS Code extension with the URL\n",
    "4. Submit jobs from VS Code\n",
    "\n",
    "**Note:** Free tier may disconnect after 12 hours. Use Colab Pro for more stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q fastapi uvicorn pyngrok numba cupy-cuda11x nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "from numba import cuda\n",
    "import cupy as cp\n",
    "\n",
    "if not cuda.is_available():\n",
    "    print(\"‚ùå CUDA not available! Enable GPU in Runtime > Change runtime type\")\n",
    "else:\n",
    "    device = cuda.get_current_device()\n",
    "    print(f\"‚úÖ GPU Available: {device.name.decode('utf-8')}\")\n",
    "    print(f\"   Memory: {device.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Compute Capability: {device.compute_capability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload GPU Engine Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload numba_engine.py from your local machine\n",
    "# Option 1: Use Colab file upload\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload numba_engine.py file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify upload\n",
    "import os\n",
    "if 'numba_engine.py' in os.listdir('.'):\n",
    "    print(\"‚úÖ numba_engine.py uploaded successfully\")\n",
    "else:\n",
    "    print(\"‚ùå numba_engine.py not found. Please upload it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download from GitHub (if you've committed the code)\n",
    "# !wget https://raw.githubusercontent.com/themitchelli/LiveCalc/main/livecalc-engines/gpu/numba_engine.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastAPI Server Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import GPU engine\n",
    "from numba_engine import (\n",
    "    NumbaGPUEngine, Policy, ProjectionConfig, ExpenseAssumptions,\n",
    "    Gender, ProductType, UnderwritingClass\n",
    ")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"LiveCalc GPU API\",\n",
    "    description=\"GPU-accelerated actuarial projection API running on Google Colab\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "\n",
    "# Enable CORS for VS Code extension\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Initialize GPU engine\n",
    "engine = NumbaGPUEngine()\n",
    "print(f\"‚úÖ GPU Engine initialized: {engine.get_schema()['gpu_model']}\")\n",
    "\n",
    "# Job storage (in-memory)\n",
    "jobs: Dict[str, Dict] = {}\n",
    "JOB_TIMEOUT_SECONDS = 15 * 60  # 15 minutes\n",
    "RESULT_RETENTION_SECONDS = 60 * 60  # 1 hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for request/response\n",
    "\n",
    "class PolicyData(BaseModel):\n",
    "    policy_id: int\n",
    "    age: int\n",
    "    gender: int  # 0=Male, 1=Female\n",
    "    sum_assured: float\n",
    "    premium: float\n",
    "    term: int\n",
    "    product_type: int = 0  # 0=Term\n",
    "    underwriting_class: int = 0  # 0=Standard\n",
    "\n",
    "class JobSubmitRequest(BaseModel):\n",
    "    policies: List[PolicyData]\n",
    "    scenarios: List[List[float]]  # num_scenarios √ó 50 years\n",
    "    mortality_table: List[List[float]]  # 2 √ó 121\n",
    "    lapse_table: List[float]  # 50 years\n",
    "    expenses: Dict[str, float]\n",
    "    config: Optional[Dict[str, float]] = None\n",
    "\n",
    "class JobSubmitResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    submitted_at: str\n",
    "    num_policies: int\n",
    "    num_scenarios: int\n",
    "\n",
    "class JobStatusResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str  # 'queued', 'running', 'completed', 'failed'\n",
    "    submitted_at: str\n",
    "    started_at: Optional[str] = None\n",
    "    completed_at: Optional[str] = None\n",
    "    progress: float = 0.0  # 0.0 to 1.0\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class JobResultResponse(BaseModel):\n",
    "    job_id: str\n",
    "    status: str\n",
    "    npvs: List[List[float]]  # num_policies √ó num_scenarios\n",
    "    statistics: Dict[str, float]\n",
    "    timing: Dict[str, float]\n",
    "    gpu_model: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background job processor\n",
    "\n",
    "def process_job(job_id: str):\n",
    "    \"\"\"Process a projection job in background\"\"\"\n",
    "    try:\n",
    "        job = jobs[job_id]\n",
    "        job['status'] = 'running'\n",
    "        job['started_at'] = datetime.utcnow().isoformat()\n",
    "        \n",
    "        # Parse input data\n",
    "        policies = [\n",
    "            Policy(\n",
    "                policy_id=p['policy_id'],\n",
    "                age=p['age'],\n",
    "                gender=Gender(p['gender']),\n",
    "                sum_assured=p['sum_assured'],\n",
    "                premium=p['premium'],\n",
    "                term=p['term'],\n",
    "                product_type=ProductType(p.get('product_type', 0)),\n",
    "                underwriting_class=UnderwritingClass(p.get('underwriting_class', 0))\n",
    "            )\n",
    "            for p in job['request']['policies']\n",
    "        ]\n",
    "        \n",
    "        scenarios = np.array(job['request']['scenarios'], dtype=np.float64)\n",
    "        mortality_table = np.array(job['request']['mortality_table'], dtype=np.float64)\n",
    "        lapse_table = np.array(job['request']['lapse_table'], dtype=np.float64)\n",
    "        \n",
    "        exp = job['request']['expenses']\n",
    "        expenses = ExpenseAssumptions(\n",
    "            per_policy_acquisition=exp['per_policy_acquisition'],\n",
    "            per_policy_maintenance=exp['per_policy_maintenance'],\n",
    "            percent_of_premium=exp['percent_of_premium'],\n",
    "            claim_expense=exp['claim_expense']\n",
    "        )\n",
    "        \n",
    "        config_data = job['request'].get('config', {})\n",
    "        config = ProjectionConfig(\n",
    "            detailed_cashflows=False,\n",
    "            mortality_multiplier=config_data.get('mortality_multiplier', 1.0),\n",
    "            lapse_multiplier=config_data.get('lapse_multiplier', 1.0),\n",
    "            expense_multiplier=config_data.get('expense_multiplier', 1.0)\n",
    "        )\n",
    "        \n",
    "        # Run projection\n",
    "        job['progress'] = 0.5\n",
    "        result = engine.project(policies, scenarios, mortality_table, lapse_table, expenses, config)\n",
    "        \n",
    "        # Store results\n",
    "        job['status'] = 'completed'\n",
    "        job['completed_at'] = datetime.utcnow().isoformat()\n",
    "        job['progress'] = 1.0\n",
    "        job['result'] = {\n",
    "            'npvs': result.npvs.tolist(),\n",
    "            'statistics': {\n",
    "                'mean': float(np.mean(result.npvs)),\n",
    "                'std': float(np.std(result.npvs)),\n",
    "                'min': float(np.min(result.npvs)),\n",
    "                'max': float(np.max(result.npvs)),\n",
    "                'median': float(np.median(result.npvs))\n",
    "            },\n",
    "            'timing': {\n",
    "                'total_runtime': result.total_runtime,\n",
    "                'kernel_time': result.kernel_time,\n",
    "                'memory_transfer_time': result.memory_transfer_time\n",
    "            },\n",
    "            'gpu_model': engine.get_schema()['gpu_model']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        job['status'] = 'failed'\n",
    "        job['completed_at'] = datetime.utcnow().isoformat()\n",
    "        job['error'] = str(e)\n",
    "        print(f\"Job {job_id} failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Endpoints\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"service\": \"LiveCalc GPU API\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"status\": \"running\",\n",
    "        \"gpu\": engine.get_schema()['gpu_model']\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    schema = engine.get_schema()\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"gpu_model\": schema['gpu_model'],\n",
    "        \"gpu_memory_gb\": schema['gpu_memory_gb'],\n",
    "        \"compute_capability\": schema['compute_capability'],\n",
    "        \"active_jobs\": sum(1 for j in jobs.values() if j['status'] in ['queued', 'running']),\n",
    "        \"total_jobs\": len(jobs)\n",
    "    }\n",
    "\n",
    "@app.post(\"/submit\", response_model=JobSubmitResponse)\n",
    "async def submit_job(request: JobSubmitRequest, background_tasks: BackgroundTasks):\n",
    "    \"\"\"Submit a new projection job\"\"\"\n",
    "    job_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Create job record\n",
    "    job = {\n",
    "        'job_id': job_id,\n",
    "        'status': 'queued',\n",
    "        'submitted_at': datetime.utcnow().isoformat(),\n",
    "        'started_at': None,\n",
    "        'completed_at': None,\n",
    "        'progress': 0.0,\n",
    "        'request': request.dict(),\n",
    "        'result': None,\n",
    "        'error': None\n",
    "    }\n",
    "    jobs[job_id] = job\n",
    "    \n",
    "    # Schedule background processing\n",
    "    background_tasks.add_task(process_job, job_id)\n",
    "    \n",
    "    return JobSubmitResponse(\n",
    "        job_id=job_id,\n",
    "        status='queued',\n",
    "        submitted_at=job['submitted_at'],\n",
    "        num_policies=len(request.policies),\n",
    "        num_scenarios=len(request.scenarios)\n",
    "    )\n",
    "\n",
    "@app.get(\"/status/{job_id}\", response_model=JobStatusResponse)\n",
    "async def get_job_status(job_id: str):\n",
    "    \"\"\"Get status of a job\"\"\"\n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    \n",
    "    job = jobs[job_id]\n",
    "    return JobStatusResponse(\n",
    "        job_id=job_id,\n",
    "        status=job['status'],\n",
    "        submitted_at=job['submitted_at'],\n",
    "        started_at=job.get('started_at'),\n",
    "        completed_at=job.get('completed_at'),\n",
    "        progress=job['progress'],\n",
    "        error=job.get('error')\n",
    "    )\n",
    "\n",
    "@app.get(\"/results/{job_id}\", response_model=JobResultResponse)\n",
    "async def get_job_results(job_id: str):\n",
    "    \"\"\"Get results of a completed job\"\"\"\n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    \n",
    "    job = jobs[job_id]\n",
    "    \n",
    "    if job['status'] != 'completed':\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Job not completed yet. Status: {job['status']}\"\n",
    "        )\n",
    "    \n",
    "    result = job['result']\n",
    "    return JobResultResponse(\n",
    "        job_id=job_id,\n",
    "        status=job['status'],\n",
    "        npvs=result['npvs'],\n",
    "        statistics=result['statistics'],\n",
    "        timing=result['timing'],\n",
    "        gpu_model=result['gpu_model']\n",
    "    )\n",
    "\n",
    "@app.delete(\"/job/{job_id}\")\n",
    "async def cancel_job(job_id: str):\n",
    "    \"\"\"Cancel a job (if not yet completed)\"\"\"\n",
    "    if job_id not in jobs:\n",
    "        raise HTTPException(status_code=404, detail=\"Job not found\")\n",
    "    \n",
    "    job = jobs[job_id]\n",
    "    \n",
    "    if job['status'] in ['queued', 'running']:\n",
    "        job['status'] = 'cancelled'\n",
    "        job['completed_at'] = datetime.utcnow().isoformat()\n",
    "        return {\"message\": \"Job cancelled\", \"job_id\": job_id}\n",
    "    else:\n",
    "        return {\"message\": f\"Job cannot be cancelled (status: {job['status']})\", \"job_id\": job_id}\n",
    "\n",
    "print(\"‚úÖ API endpoints defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start ngrok Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested event loops (required for Colab)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Optional: Set ngrok auth token for persistent URLs (requires free ngrok account)\n",
    "# Get token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "# ngrok.set_auth_token(\"YOUR_AUTH_TOKEN_HERE\")\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ LiveCalc GPU API Server Running\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüì° Public URL: {public_url}\")\n",
    "print(f\"\\nüîß Configure VS Code with this URL:\")\n",
    "print(f\"   livecalc.colabApiUrl = \\\"{public_url}\\\"\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nEndpoints:\")\n",
    "print(f\"  GET  {public_url}/              - Root\")\n",
    "print(f\"  GET  {public_url}/health        - Health check\")\n",
    "print(f\"  POST {public_url}/submit        - Submit job\")\n",
    "print(f\"  GET  {public_url}/status/{{id}}  - Job status\")\n",
    "print(f\"  GET  {public_url}/results/{{id}} - Job results\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "# Run server in background thread\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"‚úÖ Server started in background thread\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this notebook running to maintain the API server\")\n",
    "print(\"‚ö†Ô∏è  Colab free tier may disconnect after 12 hours of inactivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Get base URL\n",
    "base_url = str(public_url)\n",
    "\n",
    "# Test health endpoint\n",
    "print(\"Testing /health endpoint...\")\n",
    "response = requests.get(f\"{base_url}/health\")\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\")\n",
    "\n",
    "# Test job submission with sample data\n",
    "print(\"\\nTesting job submission...\")\n",
    "\n",
    "sample_job = {\n",
    "    \"policies\": [\n",
    "        {\n",
    "            \"policy_id\": 1,\n",
    "            \"age\": 30,\n",
    "            \"gender\": 0,\n",
    "            \"sum_assured\": 100000.0,\n",
    "            \"premium\": 500.0,\n",
    "            \"term\": 20,\n",
    "            \"product_type\": 0,\n",
    "            \"underwriting_class\": 0\n",
    "        }\n",
    "    ],\n",
    "    \"scenarios\": [[0.03] * 50 for _ in range(10)],  # 10 scenarios, 3% constant\n",
    "    \"mortality_table\": [[i/1000 for i in range(121)] for _ in range(2)],  # Simple mortality\n",
    "    \"lapse_table\": [0.05] * 50,  # 5% constant lapse\n",
    "    \"expenses\": {\n",
    "        \"per_policy_acquisition\": 100.0,\n",
    "        \"per_policy_maintenance\": 10.0,\n",
    "        \"percent_of_premium\": 0.05,\n",
    "        \"claim_expense\": 50.0\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{base_url}/submit\", json=sample_job)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "job_data = response.json()\n",
    "print(f\"Job ID: {job_data['job_id']}\")\n",
    "\n",
    "# Poll for completion\n",
    "job_id = job_data['job_id']\n",
    "print(f\"\\nPolling job status...\")\n",
    "for i in range(30):\n",
    "    time.sleep(1)\n",
    "    response = requests.get(f\"{base_url}/status/{job_id}\")\n",
    "    status_data = response.json()\n",
    "    print(f\"  [{i+1}s] Status: {status_data['status']}, Progress: {status_data['progress']:.0%}\")\n",
    "    \n",
    "    if status_data['status'] == 'completed':\n",
    "        # Get results\n",
    "        response = requests.get(f\"{base_url}/results/{job_id}\")\n",
    "        results = response.json()\n",
    "        print(f\"\\n‚úÖ Job completed!\")\n",
    "        print(f\"   Mean NPV: ${results['statistics']['mean']:,.2f}\")\n",
    "        print(f\"   Runtime: {results['timing']['total_runtime']:.3f}s\")\n",
    "        print(f\"   GPU: {results['gpu_model']}\")\n",
    "        break\n",
    "    elif status_data['status'] == 'failed':\n",
    "        print(f\"\\n‚ùå Job failed: {status_data['error']}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Timeout waiting for job completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor Server (Keep Running)\n",
    "\n",
    "This cell keeps the notebook alive and displays server status.\n",
    "Run this cell and leave it running to maintain the API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üü¢ Server Running - Keep this cell executing\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPublic URL: {public_url}\")\n",
    "print(f\"\\nPress Stop button to shutdown server\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Display current stats\n",
    "        active_jobs = sum(1 for j in jobs.values() if j['status'] in ['queued', 'running'])\n",
    "        completed_jobs = sum(1 for j in jobs.values() if j['status'] == 'completed')\n",
    "        failed_jobs = sum(1 for j in jobs.values() if j['status'] == 'failed')\n",
    "        \n",
    "        print(f\"\\rActive: {active_jobs} | Completed: {completed_jobs} | Failed: {failed_jobs} | Total: {len(jobs)}\", end=\"\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nüõë Server stopped by user\")\n",
    "    ngrok.disconnect(public_url)\n",
    "    print(\"‚úÖ Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
